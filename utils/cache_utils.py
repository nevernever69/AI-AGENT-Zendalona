import logging
from typing import Tuple, List, Optional
from langchain.schema import Document
from config import settings
from utils.chroma_utils import get_chroma_db

# Setup logging
logging.basicConfig(filename=settings.log_path, level=logging.INFO)
logger = logging.getLogger(__name__)

CACHE_COLLECTION_NAME = "zendalona_cache"
SIMILARITY_THRESHOLD = 0.65

def add_to_cache(question: str, answer: str, source: str = "manual") -> bool:
    """
    Add a question-answer pair to the cache
    
    Args:
        question: The user's question
        answer: The response to be cached
        source: Source of the answer (e.g., 'manual', 'gemini')
        
    Returns:
        bool: True if successful, False otherwise
    """
    try:
        db = get_chroma_db(collection_name=CACHE_COLLECTION_NAME)
        
        # Create a document for the Q&A pair
        doc = Document(
            page_content=question,
            metadata={
                "answer": answer,
                "source": source,
                "cached": True
            }
        )
        
        # Add to ChromaDB
        db.add_documents([doc])
        logger.info(f"Added question to cache: '{question[:50]}...' (source: {source})")
        return True
    except Exception as e:
        logger.error(f"Error adding to cache: {str(e)}")
        return False

def get_from_cache(question: str) -> Tuple[bool, str, List[str]]:
    """
    Try to find a cached answer for a similar question
    
    Args:
        question: The user's question
        
    Returns:
        Tuple of (found, answer, sources):
            found: Whether a suitable match was found
            answer: The cached answer (empty if not found)
            sources: List of source references (empty if not found)
    """
    try:
        db = get_chroma_db(collection_name=CACHE_COLLECTION_NAME)
        
        # Query the database for similar questions
        results = db.similarity_search_with_score(
            question,
            k=1  # Get the closest match
        )
        
        # Check if we got any results and if the similarity score is high enough
        if results:
            doc, score = results[0]
            similarity = 1.0 - score
            if similarity >= SIMILARITY_THRESHOLD:
                similarity_percent = (1.0 - score) * 100
            
                logger.info(f"Cache hit for question: '{question[:50]}...' (similarity: {similarity_percent:.2f}%)")
                answer = doc.metadata.get("answer", "")
                source = doc.metadata.get("source", "unknown")
            
                # Return the cached answer with metadata
                return True, answer, [f"[CACHED - {source} - similarity: {similarity_percent:.2f}%]"]
            else:
                logger.info(f"Cache miss for question: '{question[:50]}...'")
                return False, "", []
    except Exception as e:
        logger.error(f"Error querying cache: {str(e)}")
        return False, "", []

def cache_chatbot_response(question: str, answer: str, sources: List[str]) -> None:
    """
    Cache a chatbot response after it's generated by Gemini
    
    Args:
        question: The original question
        answer: The generated answer
        sources: List of sources used
    """
    try:
        # Only cache responses with actual content
        if question and answer:
            add_to_cache(question, answer, source="gemini")
            logger.info(f"Cached Gemini response for: '{question[:50]}...'")
    except Exception as e:
        logger.error(f"Error caching chatbot response: {str(e)}")